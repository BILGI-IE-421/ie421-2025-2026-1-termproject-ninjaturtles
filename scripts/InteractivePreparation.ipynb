{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ec2abce4-b973-441e-b93e-89a57e6c41a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“‚ '*TripData.csv' ÅŸablonuna uyan dosyalar aranÄ±yor...\n",
      "âœ… Toplam 12 adet dosya bulundu: ['2025_January_TripData.csv', '2025_April_TripData.csv', '2024_December_TripData.csv'] ... (ve diÄŸerleri)\n",
      "   -> 2025_January_TripData.csv dosyasÄ±ndan koordinatlar alÄ±ndÄ±.\n",
      "   -> 2025_April_TripData.csv dosyasÄ±ndan koordinatlar alÄ±ndÄ±.\n",
      "   -> 2024_December_TripData.csv dosyasÄ±ndan koordinatlar alÄ±ndÄ±.\n",
      "   -> 2024_November_TripData.csv dosyasÄ±ndan koordinatlar alÄ±ndÄ±.\n",
      "   -> 2025_February_TripData.csv dosyasÄ±ndan koordinatlar alÄ±ndÄ±.\n",
      "   -> 2025_May_TripData.csv dosyasÄ±ndan koordinatlar alÄ±ndÄ±.\n",
      "   -> 2025_July_TripData.csv dosyasÄ±ndan koordinatlar alÄ±ndÄ±.\n",
      "   -> 2025_June_TripData.csv dosyasÄ±ndan koordinatlar alÄ±ndÄ±.\n",
      "   -> 2025_October_TripData.csv dosyasÄ±ndan koordinatlar alÄ±ndÄ±.\n",
      "   -> 2025_August_TripData.csv dosyasÄ±ndan koordinatlar alÄ±ndÄ±.\n",
      "   -> 2025_September_TripData.csv dosyasÄ±ndan koordinatlar alÄ±ndÄ±.\n",
      "   -> 2025_March_TripData.csv dosyasÄ±ndan koordinatlar alÄ±ndÄ±.\n",
      "ğŸ§© TÃ¼m koordinatlar birleÅŸtiriliyor...\n",
      "âœ… Åehirdeki toplam 2226 istasyonun koordinatÄ± bulundu.\n",
      "ğŸ“Š station_stats.csv ile birleÅŸtiriliyor...\n",
      "\n",
      "ğŸ‰ TEBRÄ°KLER! 'kepler_map_data.csv' dosyan hazÄ±r.\n",
      "Toplam 2226 istasyon haritalanmaya hazÄ±r.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "\n",
    "# 1. AYARLAR: Dosya Ä°sim Åablonu\n",
    "# Dosya isimlerin \"...TripData\" ÅŸeklinde bitiyor.\n",
    "# EÄŸer dosyalarÄ±nÄ±n uzantÄ±sÄ± .csv ise \"*TripData.csv\" kullan.\n",
    "# EÄŸer uzantÄ± gÃ¶rÃ¼nmÃ¼yorsa \"*TripData*\" da Ã§alÄ±ÅŸÄ±r.\n",
    "FILE_PATTERN = \"*TripData.csv\" \n",
    "\n",
    "print(f\"ğŸ“‚ '{FILE_PATTERN}' ÅŸablonuna uyan dosyalar aranÄ±yor...\")\n",
    "\n",
    "# DosyalarÄ± listele\n",
    "files = glob.glob(FILE_PATTERN)\n",
    "\n",
    "# Listeyi kontrol et: EÄŸer boÅŸsa uzantÄ±sÄ±z dene\n",
    "if not files:\n",
    "    print(\"âš ï¸ .csv uzantÄ±sÄ±yla dosya bulunamadÄ±, uzantÄ±sÄ±z aranÄ±yor...\")\n",
    "    FILE_PATTERN = \"*TripData*\"\n",
    "    files = glob.glob(FILE_PATTERN)\n",
    "\n",
    "print(f\"âœ… Toplam {len(files)} adet dosya bulundu: {files[:3]} ... (ve diÄŸerleri)\")\n",
    "\n",
    "# KoordinatlarÄ± biriktireceÄŸimiz liste\n",
    "all_coords = []\n",
    "\n",
    "# 2. DÃ–NGÃœ: DosyalarÄ± Tek Tek Oku (RAM Dostu)\n",
    "for f in files:\n",
    "    try:\n",
    "        # Sadece gerekli sÃ¼tunlarÄ± okuyoruz (Ã‡ok hÄ±zlÄ±)\n",
    "        # Dosyanda sÃ¼tun adlarÄ± 'start_station_name', 'start_lat', 'start_lng' olmalÄ±.\n",
    "        df_chunk = pd.read_csv(f, usecols=['start_station_name', 'start_lat', 'start_lng'])\n",
    "        \n",
    "        # Bu dosyadaki benzersiz istasyonlarÄ± al\n",
    "        unique_stations = df_chunk.drop_duplicates('start_station_name')\n",
    "        \n",
    "        # Listeye ekle\n",
    "        all_coords.append(unique_stations)\n",
    "        \n",
    "        # Bilgi ver\n",
    "        print(f\"   -> {f} dosyasÄ±ndan koordinatlar alÄ±ndÄ±.\")\n",
    "        \n",
    "    except ValueError:\n",
    "        print(f\"   âš ï¸ UYARI: {f} dosyasÄ±nda beklenen sÃ¼tunlar (start_station_name vs.) bulunamadÄ±, geÃ§iliyor.\")\n",
    "    except Exception as e:\n",
    "        print(f\"   âŒ HATA: {f} okunurken sorun oluÅŸtu: {e}\")\n",
    "\n",
    "# 3. ADRES DEFTERÄ°NÄ° BÄ°RLEÅTÄ°R\n",
    "if all_coords:\n",
    "    print(\"ğŸ§© TÃ¼m koordinatlar birleÅŸtiriliyor...\")\n",
    "    master_coords = pd.concat(all_coords)\n",
    "    \n",
    "    # Tekrar edenleri sil (AynÄ± istasyon KasÄ±m'da da AralÄ±k'ta da vardÄ±r, tekini al)\n",
    "    master_coords = master_coords.drop_duplicates('start_station_name')\n",
    "    print(f\"âœ… Åehirdeki toplam {len(master_coords)} istasyonun koordinatÄ± bulundu.\")\n",
    "    \n",
    "    # 4. Ä°STATÄ°STÄ°KLERLE EÅLEÅTÄ°R (Denge Verisi)\n",
    "    print(\"ğŸ“Š station_stats.csv ile birleÅŸtiriliyor...\")\n",
    "    df_stats = pd.read_csv(\"station_stats.csv\")\n",
    "    \n",
    "    # SayÄ± formatÄ±nÄ± temizle (Bar charttaki gibi)\n",
    "    df_stats['Net_Balance'] = df_stats['Net_Balance'].astype(str).str.replace(',', '')\n",
    "    df_stats['Net_Balance'] = pd.to_numeric(df_stats['Net_Balance'], errors='coerce').fillna(0)\n",
    "    \n",
    "    # Grupla\n",
    "    df_grouped_stats = df_stats.groupby('start_station_name', as_index=False)['Net_Balance'].sum()\n",
    "    \n",
    "    # BÄ°RLEÅTÄ°R (Left Merge) -> Sadece istatistiÄŸi olanlarÄ± haritaya koyacaÄŸÄ±z\n",
    "    map_data = pd.merge(df_grouped_stats, master_coords, on='start_station_name', how='left')\n",
    "    \n",
    "    # KoordinatÄ± bulunamayanlarÄ± (Ã¶rn: yeni aÃ§Ä±lan veya adÄ± deÄŸiÅŸen) temizle\n",
    "    map_data = map_data.dropna(subset=['start_lat', 'start_lng'])\n",
    "    \n",
    "    # 5. DOSYAYI KAYDET\n",
    "    map_data.to_csv(\"kepler_map_data.csv\", index=False)\n",
    "    print(f\"\\nğŸ‰ TEBRÄ°KLER! 'kepler_map_data.csv' dosyan hazÄ±r.\")\n",
    "    print(f\"Toplam {len(map_data)} istasyon haritalanmaya hazÄ±r.\")\n",
    "    \n",
    "else:\n",
    "    print(\"âŒ HATA: HiÃ§bir dosyadan veri okunamadÄ±. KlasÃ¶rde '*TripData' isminde dosyalar olduÄŸundan emin misin?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8fc14969-623b-4844-8ae6-ec2d2f744f97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š station_stats.csv okunuyor...\n",
      "   -> Ä°statistiklerde en dÃ¼ÅŸÃ¼k bakiye: -72696.0\n",
      "ğŸ“‚ Koordinatlar '*TripData.csv' dosyalarÄ±ndan toplanÄ±yor...\n",
      "âœ… Toplam 2225 istasyonun adresi bulundu.\n",
      "--------------------------------------------------\n",
      "ğŸ‰ DOSYA HAZIR: 'kepler_map_data_fixed.csv'\n",
      "Toplam Ä°stasyon SayÄ±sÄ±: 2225\n",
      "âœ… Dosyadaki En DÃ¼ÅŸÃ¼k Bakiye (Source): -72696.0\n",
      "ğŸš€ BAÅARILI! -72k'lÄ±k istasyon artÄ±k dosyada mevcut.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "\n",
    "# 1. AYARLAR\n",
    "FILE_PATTERN = \"*TripData.csv\" # Dosya ÅŸablonun\n",
    "\n",
    "# 2. Ã–NCE Ä°STATÄ°STÄ°KLERÄ° OKU VE TEMÄ°ZLE\n",
    "print(\"ğŸ“Š station_stats.csv okunuyor...\")\n",
    "df_stats = pd.read_csv(\"station_stats.csv\")\n",
    "\n",
    "# SayÄ± formatÄ± temizliÄŸi\n",
    "df_stats['Net_Balance'] = df_stats['Net_Balance'].astype(str).str.replace(',', '')\n",
    "df_stats['Net_Balance'] = pd.to_numeric(df_stats['Net_Balance'], errors='coerce').fillna(0)\n",
    "\n",
    "# --- KRÄ°TÄ°K HAMLE 1: Ä°sim TemizliÄŸi (TraÅŸlama) ---\n",
    "# Ä°simlerin baÅŸÄ±ndaki/sonundaki boÅŸluklarÄ± sil ki eÅŸleÅŸme hatasÄ± olmasÄ±n.\n",
    "df_stats['start_station_name'] = df_stats['start_station_name'].astype(str).str.strip()\n",
    "\n",
    "# Gruplama\n",
    "df_grouped_stats = df_stats.groupby('start_station_name', as_index=False)['Net_Balance'].sum()\n",
    "\n",
    "print(f\"   -> Ä°statistiklerde en dÃ¼ÅŸÃ¼k bakiye: {df_grouped_stats['Net_Balance'].min()}\")\n",
    "# Burada -72k civarÄ± bir sayÄ± gÃ¶rmelisin.\n",
    "\n",
    "# 3. KOORDÄ°NATLARI TOPLA (DÃ¶ngÃ¼)\n",
    "print(f\"ğŸ“‚ Koordinatlar '{FILE_PATTERN}' dosyalarÄ±ndan toplanÄ±yor...\")\n",
    "files = glob.glob(FILE_PATTERN)\n",
    "\n",
    "if not files:\n",
    "    # EÄŸer csv bulunamazsa uzantÄ±sÄ±z ara\n",
    "    files = glob.glob(\"*TripData*\")\n",
    "\n",
    "all_coords = []\n",
    "\n",
    "for f in files:\n",
    "    try:\n",
    "        # Sadece gerekli sÃ¼tunlarÄ± oku\n",
    "        df_chunk = pd.read_csv(f, usecols=['start_station_name', 'start_lat', 'start_lng'])\n",
    "        \n",
    "        # --- KRÄ°TÄ°K HAMLE 2: Buradaki Ä°simleri de TraÅŸla ---\n",
    "        df_chunk['start_station_name'] = df_chunk['start_station_name'].astype(str).str.strip()\n",
    "        \n",
    "        # Benzersizleri al\n",
    "        unique_stations = df_chunk.drop_duplicates('start_station_name')\n",
    "        all_coords.append(unique_stations)\n",
    "        \n",
    "    except ValueError:\n",
    "        pass # SÃ¼tun yoksa geÃ§\n",
    "\n",
    "# 4. KOORDÄ°NATLARI BÄ°RLEÅTÄ°R\n",
    "if all_coords:\n",
    "    master_coords = pd.concat(all_coords)\n",
    "    # TekrarlarÄ± sil (Enlem/Boylam deÄŸiÅŸmez, ilkini al yeter)\n",
    "    master_coords = master_coords.drop_duplicates('start_station_name')\n",
    "    \n",
    "    print(f\"âœ… Toplam {len(master_coords)} istasyonun adresi bulundu.\")\n",
    "    \n",
    "    # 5. FÄ°NAL BÄ°RLEÅTÄ°RME (MERGE)\n",
    "    # how='inner' yerine 'left' kullanÄ±yoruz ki istatistiÄŸi olup koordinatÄ± olmayanlarÄ± gÃ¶relim\n",
    "    map_data = pd.merge(df_grouped_stats, master_coords, on='start_station_name', how='left')\n",
    "    \n",
    "    # KONTROL: KoordinatÄ± bulunamayan \"Ã¶nemli\" istasyon var mÄ±?\n",
    "    missing = map_data[map_data['start_lat'].isnull()]\n",
    "    if not missing.empty:\n",
    "        print(f\"âš ï¸ DÄ°KKAT: {len(missing)} istasyonun koordinatÄ± bulunamadÄ±!\")\n",
    "        print(\"   -> KoordinatÄ± bulunamayan en bÃ¼yÃ¼k istasyonlar:\")\n",
    "        print(missing.sort_values('Net_Balance').head(3))\n",
    "    \n",
    "    # KoordinatÄ± olanlarÄ± al ve kaydet\n",
    "    map_data_final = map_data.dropna(subset=['start_lat', 'start_lng'])\n",
    "    \n",
    "    map_data_final.to_csv(\"kepler_map_data_fixed.csv\", index=False)\n",
    "    \n",
    "    print(\"-\" * 50)\n",
    "    print(f\"ğŸ‰ DOSYA HAZIR: 'kepler_map_data_fixed.csv'\")\n",
    "    print(f\"Toplam Ä°stasyon SayÄ±sÄ±: {len(map_data_final)}\")\n",
    "    \n",
    "    # KANIT: En dÃ¼ÅŸÃ¼k bakiyeli istasyonu gÃ¶ster\n",
    "    min_val = map_data_final['Net_Balance'].min()\n",
    "    print(f\"âœ… Dosyadaki En DÃ¼ÅŸÃ¼k Bakiye (Source): {min_val}\")\n",
    "    \n",
    "    if min_val < -50000:\n",
    "        print(\"ğŸš€ BAÅARILI! -72k'lÄ±k istasyon artÄ±k dosyada mevcut.\")\n",
    "    else:\n",
    "        print(\"âŒ HATA: Hala yÃ¼ksek deÄŸerler eksik. Dosya isimlerinde ciddi bir fark olabilir.\")\n",
    "\n",
    "else:\n",
    "    print(\"âŒ Dosya okunamadÄ±.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "725dd10c-54e9-4036-b086-7f119ea1faaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ¬ Animasyon iÃ§in veri sÄ±fÄ±rdan oluÅŸturuluyor (RAM Dostu Mod)...\n",
      "ğŸ“‚ 12 adet veri dosyasÄ± taranacak...\n",
      "   Reading: 2025_January_TripData.csv ...\n",
      "   Reading: 2025_April_TripData.csv ...\n",
      "   Reading: 2024_December_TripData.csv ...\n",
      "   Reading: 2024_November_TripData.csv ...\n",
      "   Reading: 2025_February_TripData.csv ...\n",
      "   Reading: 2025_May_TripData.csv ...\n",
      "   Reading: 2025_July_TripData.csv ...\n",
      "   Reading: 2025_June_TripData.csv ...\n",
      "   Reading: 2025_October_TripData.csv ...\n",
      "   Reading: 2025_August_TripData.csv ...\n",
      "   Reading: 2025_September_TripData.csv ...\n",
      "   Reading: 2025_March_TripData.csv ...\n",
      "ğŸ§© Veriler birleÅŸtiriliyor...\n",
      "âœ… Saatlik istatistikler hazÄ±r! (53720 satÄ±r)\n",
      "ğŸ“ Koordinatlar ekleniyor...\n",
      "--------------------------------------------------\n",
      "ğŸ‰ FÄ°NAL DOSYA HAZIR: 'kepler_animation_final.csv'\n",
      "Bu dosyayÄ± Kepler.gl'e yÃ¼kle ve Filter kÄ±smÄ±ndan 'real_time' seÃ§!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "\n",
    "print(\"ğŸ¬ Animasyon iÃ§in veri sÄ±fÄ±rdan oluÅŸturuluyor (RAM Dostu Mod)...\")\n",
    "\n",
    "# 1. DOSYA AYARLARI\n",
    "FILE_PATTERN = \"*TripData.csv\" \n",
    "files = glob.glob(FILE_PATTERN)\n",
    "\n",
    "if not files:\n",
    "    files = glob.glob(\"*TripData*\") # UzantÄ±sÄ±z varsa onu dene\n",
    "\n",
    "print(f\"ğŸ“‚ {len(files)} adet veri dosyasÄ± taranacak...\")\n",
    "\n",
    "# SonuÃ§larÄ± biriktireceÄŸimiz havuzlar\n",
    "total_outflows = []\n",
    "total_inflows = []\n",
    "\n",
    "# 2. DÃ–NGÃœ: Her dosyayÄ± oku ve saatlik Ã¶zetini Ã§Ä±kar\n",
    "for f in files:\n",
    "    try:\n",
    "        print(f\"   Reading: {f} ...\")\n",
    "        \n",
    "        # Sadece gerekli sÃ¼tunlarÄ± oku (HÄ±z iÃ§in)\n",
    "        # Bize Ä°sim ve Zaman lazÄ±m\n",
    "        cols = ['start_station_name', 'end_station_name', 'started_at', 'ended_at']\n",
    "        df_chunk = pd.read_csv(f, usecols=cols)\n",
    "\n",
    "        # --- GÄ°DENLER (Outflows) ---\n",
    "        # Saati Ã§ek (HÄ±zlÄ± yÃ¶ntem: String kesme) \"2024-01-01 14:30:00\" -> \"14\"\n",
    "        df_chunk['start_hour'] = df_chunk['started_at'].astype(str).str[11:13]\n",
    "        \n",
    "        # Grupla: Hangi istasyondan, hangi saatte kaÃ§ kiÅŸi Ã§Ä±ktÄ±?\n",
    "        out_counts = df_chunk.groupby(['start_station_name', 'start_hour']).size().rename('Out_Count')\n",
    "        total_outflows.append(out_counts)\n",
    "\n",
    "        # --- GELENLER (Inflows) ---\n",
    "        df_chunk['end_hour'] = df_chunk['ended_at'].astype(str).str[11:13]\n",
    "        \n",
    "        # Grupla: Hangi istasyona, hangi saatte kaÃ§ kiÅŸi geldi?\n",
    "        in_counts = df_chunk.groupby(['end_station_name', 'end_hour']).size().rename('In_Count')\n",
    "        total_inflows.append(in_counts)\n",
    "\n",
    "        # Temizlik\n",
    "        del df_chunk\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"   âš ï¸ Hata ({f}): {e}\")\n",
    "\n",
    "# 3. TÃœM AYLARI BÄ°RLEÅTÄ°R\n",
    "print(\"ğŸ§© Veriler birleÅŸtiriliyor...\")\n",
    "\n",
    "# TÃ¼m gidenleri topla (Ä°sim ve Saate gÃ¶re)\n",
    "all_out = pd.concat(total_outflows).groupby(level=[0, 1]).sum().reset_index()\n",
    "all_out.columns = ['station_name', 'hour', 'Out_Count']\n",
    "\n",
    "# TÃ¼m gelenleri topla\n",
    "all_in = pd.concat(total_inflows).groupby(level=[0, 1]).sum().reset_index()\n",
    "all_in.columns = ['station_name', 'hour', 'In_Count']\n",
    "\n",
    "# 4. NET DENGE HESABI (Giren - Ã‡Ä±kan)\n",
    "# Ä°ki tabloyu birleÅŸtir\n",
    "df_hourly = pd.merge(all_out, all_in, on=['station_name', 'hour'], how='outer').fillna(0)\n",
    "\n",
    "# Net Denge\n",
    "df_hourly['Net_Balance'] = df_hourly['In_Count'] - df_hourly['Out_Count']\n",
    "df_hourly['Abs_Balance'] = df_hourly['Net_Balance'].abs() # YÃ¼kseklik iÃ§in\n",
    "\n",
    "print(f\"âœ… Saatlik istatistikler hazÄ±r! ({len(df_hourly)} satÄ±r)\")\n",
    "\n",
    "# 5. KOORDÄ°NATLARI EKLE (Kepler Ä°Ã§in Åart)\n",
    "print(\"ğŸ“ Koordinatlar ekleniyor...\")\n",
    "df_coords = pd.read_csv(\"kepler_map_data_fixed.csv\") # Az Ã¶nce dÃ¼zelttiÄŸimiz dosya\n",
    "\n",
    "# Ä°sim temizliÄŸi (EÅŸleÅŸme iÃ§in)\n",
    "df_hourly['station_name'] = df_hourly['station_name'].astype(str).str.strip()\n",
    "df_coords['start_station_name'] = df_coords['start_station_name'].astype(str).str.strip()\n",
    "\n",
    "# KoordinatlarÄ± Ã§ek\n",
    "final_df = pd.merge(df_hourly, df_coords[['start_station_name', 'start_lat', 'start_lng']], \n",
    "                    left_on='station_name', right_on='start_station_name', how='inner')\n",
    "\n",
    "# 6. KEPLER Ä°Ã‡Ä°N ZAMAN FORMATI (Slider Ä°Ã§in)\n",
    "# Sahte bir tarih ekleyerek saati Kepler'in anlayacaÄŸÄ± formata Ã§evir\n",
    "# Ã–rn: Saat 14 -> \"2024-01-01 14:00\"\n",
    "final_df['real_time'] = \"2024-01-01 \" + final_df['hour'].astype(str).str.zfill(2) + \":00\"\n",
    "\n",
    "# Gereksiz sÃ¼tunlarÄ± temizle ve kaydet\n",
    "columns_to_save = ['station_name', 'real_time', 'Net_Balance', 'Abs_Balance', 'start_lat', 'start_lng']\n",
    "final_df = final_df[columns_to_save]\n",
    "\n",
    "final_df.to_csv(\"kepler_animation_final.csv\", index=False)\n",
    "\n",
    "print(\"-\" * 50)\n",
    "print(f\"ğŸ‰ FÄ°NAL DOSYA HAZIR: 'kepler_animation_final.csv'\")\n",
    "print(\"Bu dosyayÄ± Kepler.gl'e yÃ¼kle ve Filter kÄ±smÄ±ndan 'real_time' seÃ§!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ec97d001-fd23-48c5-a27d-e6a4fbb948f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ¬ Animasyon yumuÅŸatÄ±lÄ±yor (Interpolasyon)...\n",
      "âœ… Orijinal dosya okundu: 53175 satÄ±r.\n",
      "â³ Veri 10'ar dakikalÄ±k aralÄ±klara geniÅŸletiliyor (Biraz sÃ¼rebilir)...\n",
      "--------------------------------------------------\n",
      "ğŸ‰ AKICI DOSYA HAZIR: 'kepler_animation_smooth_10min.csv'\n",
      "Yeni SatÄ±r SayÄ±sÄ±: 308321\n",
      "Bu dosyayÄ± Kepler.gl'e yÃ¼kleyip 'real_time' filtresi eklediÄŸinde akÄ±cÄ± bir geÃ§iÅŸ gÃ¶receksin! ğŸš€\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "print(\"ğŸ¬ Animasyon yumuÅŸatÄ±lÄ±yor (Interpolasyon)...\")\n",
    "\n",
    "# 1. MEVCUT DOSYAYI OKU\n",
    "try:\n",
    "    df = pd.read_csv(\"kepler_animation_final.csv\")\n",
    "    print(f\"âœ… Orijinal dosya okundu: {len(df)} satÄ±r.\")\n",
    "except FileNotFoundError:\n",
    "    print(\"âŒ HATA: 'kepler_animation_final.csv' bulunamadÄ±. LÃ¼tfen bir Ã¶nceki kodu Ã§alÄ±ÅŸtÄ±rÄ±p bu dosyayÄ± oluÅŸtur.\")\n",
    "    exit()\n",
    "\n",
    "# 2. ZAMAN AYARLARI\n",
    "df['real_time'] = pd.to_datetime(df['real_time'])\n",
    "\n",
    "# 3. YUMUÅATMA Ä°ÅLEMÄ° (DÃœZELTÄ°LEN KISIM)\n",
    "# '10T' yerine '10min' kullanÄ±yoruz.\n",
    "print(\"â³ Veri 10'ar dakikalÄ±k aralÄ±klara geniÅŸletiliyor (Biraz sÃ¼rebilir)...\")\n",
    "\n",
    "df_smooth = df.set_index('real_time').groupby('station_name')[['Net_Balance', 'start_lat', 'start_lng']].resample('10min').mean()\n",
    "\n",
    "# Ara deÄŸerleri doldur (Linear Interpolation)\n",
    "df_smooth['Net_Balance'] = df_smooth['Net_Balance'].interpolate(method='linear')\n",
    "\n",
    "# KoordinatlarÄ± doldur (Ã–nceki deÄŸeri kopyala)\n",
    "df_smooth['start_lat'] = df_smooth['start_lat'].ffill()\n",
    "df_smooth['start_lng'] = df_smooth['start_lng'].ffill()\n",
    "\n",
    "# Ä°ndeksi sÄ±fÄ±rla\n",
    "df_smooth = df_smooth.reset_index()\n",
    "\n",
    "# 4. YÃœKSEKLÄ°K HESABINI GÃœNCELLE\n",
    "# YumuÅŸatÄ±lan Net_Balance'a gÃ¶re yÃ¼ksekliÄŸi tekrar hesapla\n",
    "df_smooth['Abs_Balance'] = df_smooth['Net_Balance'].abs()\n",
    "\n",
    "# 5. TEMÄ°ZLÄ°K\n",
    "df_smooth = df_smooth.dropna()\n",
    "\n",
    "# 6. KAYDET\n",
    "output_file = \"kepler_animation_smooth_10min.csv\"\n",
    "df_smooth.to_csv(output_file, index=False)\n",
    "\n",
    "print(\"-\" * 50)\n",
    "print(f\"ğŸ‰ AKICI DOSYA HAZIR: '{output_file}'\")\n",
    "print(f\"Yeni SatÄ±r SayÄ±sÄ±: {len(df_smooth)}\")\n",
    "print(\"Bu dosyayÄ± Kepler.gl'e yÃ¼kleyip 'real_time' filtresi eklediÄŸinde akÄ±cÄ± bir geÃ§iÅŸ gÃ¶receksin! ğŸš€\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3a890f46-7ea1-4cbb-b6d1-691c7842c5a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ—ï¸ Ä°stasyonlar 3D Binalara dÃ¶nÃ¼ÅŸtÃ¼rÃ¼lÃ¼yor...\n",
      "--------------------------------------------------\n",
      "ğŸ‰ HAZIR: 'kepler_3d_buildings.csv' dosyasÄ± oluÅŸturuldu.\n",
      "Åimdi Kepler.gl'i aÃ§ ve bu dosyayÄ± yÃ¼kle. 'Polygon' katmanÄ±nÄ± kullanacaÄŸÄ±z!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "print(\"ğŸ—ï¸ Ä°stasyonlar 3D Binalara dÃ¶nÃ¼ÅŸtÃ¼rÃ¼lÃ¼yor...\")\n",
    "\n",
    "# 1. VERÄ°YÄ° OKU\n",
    "# En son oluÅŸturduÄŸumuz smooth (akÄ±cÄ±) dosyayÄ± kullanÄ±yoruz\n",
    "df = pd.read_csv(\"kepler_animation_smooth_10min.csv\")\n",
    "\n",
    "# 2. POLYGON OLUÅTURMA FONKSÄ°YONU\n",
    "# Her noktanÄ±n etrafÄ±na yaklaÅŸÄ±k 20 metrelik bir kare Ã§izer.\n",
    "# Lat/Lon dÃ¼nyada derece cinsindendir. 0.0002 derece yaklaÅŸÄ±k 20 metreye denk gelir.\n",
    "\n",
    "def create_square_polygon(lat, lng, size=0.0002):\n",
    "    # Karenin 4 kÃ¶ÅŸesini hesapla\n",
    "    lat_min = lat - size\n",
    "    lat_max = lat + size\n",
    "    lng_min = lng - size\n",
    "    lng_max = lng + size\n",
    "    \n",
    "    # WKT (Well Known Text) formatÄ±nda Polygon stringi oluÅŸtur\n",
    "    # Format: POLYGON ((lng1 lat1, lng2 lat1, lng2 lat2, lng1 lat2, lng1 lat1))\n",
    "    return f\"POLYGON (({lng_min} {lat_min}, {lng_max} {lat_min}, {lng_max} {lat_max}, {lng_min} {lat_max}, {lng_min} {lat_min}))\"\n",
    "\n",
    "# 3. GEOMETRÄ° SÃœTUNU EKLE\n",
    "# Apply fonksiyonu ile her satÄ±r iÃ§in kare oluÅŸturuyoruz\n",
    "df['Geometry'] = df.apply(lambda row: create_square_polygon(row['start_lat'], row['start_lng']), axis=1)\n",
    "\n",
    "# 4. RENK Ä°Ã‡Ä°N STRÄ°NG AYARI (Opsiyonel ama garantidir)\n",
    "# Bazen Kepler sayÄ±sal renklendirmede hata yapabilir, ama ÅŸu an gerek yok, Net_Balance kullanacaÄŸÄ±z.\n",
    "\n",
    "# 5. KAYDET\n",
    "output_file = \"kepler_3d_buildings.csv\"\n",
    "df.to_csv(output_file, index=False)\n",
    "\n",
    "print(\"-\" * 50)\n",
    "print(f\"ğŸ‰ HAZIR: '{output_file}' dosyasÄ± oluÅŸturuldu.\")\n",
    "print(\"Åimdi Kepler.gl'i aÃ§ ve bu dosyayÄ± yÃ¼kle. 'Polygon' katmanÄ±nÄ± kullanacaÄŸÄ±z!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "814f0f86-ceeb-44a4-83b5-c703c4d9c114",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
